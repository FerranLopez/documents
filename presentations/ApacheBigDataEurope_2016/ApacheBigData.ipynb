{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Get In Control Of Your Workflows With Apache Airflow\n",
    "\n",
    "\n",
    "Christian Trebing, Apache Big Data Europe 2016, Seville\n",
    "\n",
    "@ctrebing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Imagine: \n",
    "- you are a data driven company\n",
    "- each night you get data from your customers and this data wants to be processed\n",
    "- processing happens is done in separate steps (for example booking, machine learning, decision taking)\n",
    "- if errors happen, you want to get an overview on what happened when\n",
    "- as you might have already guessed: you have a tight time schedule each night\n",
    "\n",
    "What options do you have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Doing it with cron\n",
    "- works for the start\n",
    "- only time triggers possible, no dependency\n",
    "- error handling is hard\n",
    "\n",
    "![gantt chart](images/cron_chart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![gantt chart](images/cron_chart_ideal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![gantt chart](images/cron_chart_exception.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Writing a workflow processing tool\n",
    "- we did that for the start. and not just one. \n",
    "\n",
    "- start is easy, everything is great.\n",
    "- soon you reach the limits. Then you either have to invest much more than you thought initially or live with the limits\n",
    " - some ideas: concurrency, traceability, manual triggers, external interfaces, ui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Using an open source workflow processing tool\n",
    "- we evaluated multiple ones and decided for airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why did we decide for airflow?\n",
    "- written in python. we know that and we like it.\n",
    "- also workflows are defined in python code\n",
    "- view of present and past runs, logging features\n",
    "- extensible through plugins\n",
    "- active development (apache incubator project)\n",
    "- nice ui, possibility to define a REST interface\n",
    "- relatively lightweight: two processes on a server + some database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators import BookData, Predict, Decide\n",
    "\n",
    "dag_id = \"daily_processing\"\n",
    "schedule_interval = '0 22 * * *'\n",
    "\n",
    "default_args = {\n",
    "    'retries': 2,\n",
    "    'retry_delay': timedelta(minutes=5)\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id,\n",
    "    start_date=datetime.date(2016, 16, 11),\n",
    "    schedule_interval=schedule_interval,\n",
    "    default_args=default_args)\n",
    "\n",
    "book = BookData(dag=dag)\n",
    "\n",
    "predict = Predict(dag=dag)\n",
    "predict.set_upstream(book)\n",
    "\n",
    "decide = Decide(dag=dag)\n",
    "decide.set_upstream(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![straight flow](images/straight_flow.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Fan In / Fan Out\n",
    "\n",
    "book = BookData(dag=dag)\n",
    "\n",
    "predict_ger = Predict(dag=dag, country='GER')\n",
    "predict_ger.set_upstream(book)\n",
    "\n",
    "predict_uk = Predict(dag=dag, country='UK')\n",
    "predict_uk.set_upstream(book)\n",
    "\n",
    "decide = Decide(dag=dag)\n",
    "decide.set_upstream(predict_ger)\n",
    "decide.set_upstream(predict_uk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![diamond](images/diamond.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# On the UI\n",
    "- DAG overview (start screen)\n",
    "- run view\n",
    "- tree view\n",
    "- runtimes\n",
    "- gantt chart\n",
    "- log view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# DAG Overview (start screen)\n",
    "\n",
    "![DAG Overview](images/dag_overview.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# DAG Run View\n",
    "![DAG Run](images/dag_run.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Tree View\n",
    "![DAG Run](images/dag_tree_view.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Runtimes\n",
    "![DAG Run](images/dag_task_duration.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Gantt Chart\n",
    "![DAG Run](images/dag_gantt.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Log View\n",
    "![DAG Log](images/dag_log.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Operators\n",
    "\n",
    "Many basic operators are included in airflow:\n",
    "- BashOperator\n",
    "- SimpleHttpOperator\n",
    "- PostgresOperator / SqliteOperator\n",
    "- PythonOperator\n",
    "- EmailOperator\n",
    "- ...\n",
    "\n",
    "Also there are sensors to wait for things:\n",
    "- HttpSensor\n",
    "- HdfsSensor\n",
    "- SqlSensor\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Python Operator\n",
    "\n",
    "Within your DAG definition, you can define arbitrary python code that can be run by a PythonOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def print_context(ds, **kwargs):\n",
    "    pprint(kwargs)\n",
    "    print(ds)\n",
    "    return 'Whatever you return gets printed in the logs'\n",
    "\n",
    "run_this = PythonOperator(\n",
    "    task_id='print_the_context',\n",
    "    provide_context=True,\n",
    "    python_callable=print_context,\n",
    "    dag=dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Great flexibility\n",
    "- be aware of dependencies: all imported python packages have to be available on all worker nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# But I need a different operator...\n",
    "\n",
    "![async http request](images/async_http.png)\n",
    "\n",
    "1. I could use a SimpleHttpOperator and afterwards an HttpSensor\n",
    " - would work functional wise\n",
    " - but wouldn't it be nice to see the execution time directly as the operator run time?\n",
    " \n",
    "2. Time for a new operator!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# operator implementation\n",
    "\n",
    "import time, logging\n",
    "from airflow import models, hooks\n",
    "\n",
    "class Decide(models.BaseOperator):\n",
    "    @airflow_utils.apply_defaults\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Decide, self).__init__(\n",
    "            task_id='decide',\n",
    "            **kwargs)\n",
    "        self.http_conn_id = 'DECISION_SERVER'\n",
    "        self.endpoint_job_start = 'decide/'\n",
    "        self.endpoint_job_status = 'job_status/'\n",
    "\n",
    "    def execute(self, context):\n",
    "        http = hooks.HttpHook(method='POST', http_conn_id=self.http_conn_id)\n",
    "        response = http.run(endpoint=self.endpoint_job_start)\n",
    "        job_id = response.json()['job_id']\n",
    "        logging.info('started decision job with job id {}'.format(job_id))\n",
    "        self.wait_for_job(job_id)\n",
    "\n",
    "    def wait_for_job(self, job_id):\n",
    "        job_status = None\n",
    "        http = hooks.HttpHook(method='GET', http_conn_id=self.http_conn_id)\n",
    "        while not job_status == 'FINISHED':\n",
    "            time.sleep(1)\n",
    "            response = http.run(endpoint=self.endpoint_job_status + str(job_id))\n",
    "            job_status = response.json()['status']\n",
    "            logging.info('status of decision job {} is {}'.format(job_id, job_status))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# State Handling\n",
    "\n",
    "Variables\n",
    "- per airflow instance\n",
    "\n",
    "XCOMs\n",
    "- per DAG run / task\n",
    "\n",
    "These two types of states are persisted in two database tables\n",
    "- does not get lost on scheduler restart\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Example: Resource Reservation\n",
    "\n",
    "Requirements:\n",
    "- Some of the tasks inside my DAG require exclusive access to resource\n",
    "- multiple DAGs exist that require this exclusive access\n",
    "\n",
    "Solution:\n",
    "- before each task block requiring the resource insert a new task that acquires a lock for this resource\n",
    "- after each task block requiring the resource insert a new task that returbs the lock for this resource\n",
    "- only return the lock if it has been acquired during this DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![state_handling](images/state_handling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Branching\n",
    "\n",
    "- use BranchPythonOperator\n",
    "- implement decision logic in python function\n",
    "- return value of python function is task_id to be done next\n",
    "\n",
    "example: do different processing on weekend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![branch dag](images/branch_dag.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_weekday_python():            \n",
    "    weekday = datetime.now().weekday()        \n",
    "    if weekday in [5, 6]:       \n",
    "        return 'decide_weekend' \n",
    "    else:                        \n",
    "        return 'decide'  \n",
    "                                    \n",
    "check_weekday = BranchPythonOperator( \n",
    "    task_id='check_weekday', \n",
    "    python_callable=check_weekday_python, \n",
    "    dag=dag       \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Plugin Concept\n",
    "- own operators\n",
    "- own blueprints\n",
    "- in the airflow configuration, give path to plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Plugin Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from airflow.plugins_manager import AirflowPlugin\n",
    "\n",
    "from plugins import blueprints\n",
    "from plugins import operators\n",
    "\n",
    "\n",
    "# Defining the plugin class\n",
    "class ApacheBigDataPlugin(AirflowPlugin):\n",
    "    name = \"apache_big_data_plugin\"\n",
    "    operators = [\n",
    "        operators.BookData,\n",
    "        operators.Predict,\n",
    "        operators.Decide\n",
    "    ]\n",
    "    flask_blueprints = [blueprints.TriggerBlueprint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Defining own Blueprints\n",
    "\n",
    "Extends the web server\n",
    "\n",
    "For example: currently, no REST API exists to ask trigger dags or ask for the state of a dag run\n",
    "\n",
    "you can add your own blueprints that run within the webserver and can access all airflow functionality\n",
    "- add as a flask blueprint\n",
    "- we defined endpoints for the above (trigger dags/ask for state of a dag run)\n",
    "- need to be careful of maintaining them through an airflow version upgrade\n",
    "\n",
    "for implementation, see the example git repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "curl -X POST localhost:8080/trigger/daily_processing    \n",
    "{ \"dag_id\": \"daily_processing\", \"run_id\": \"external_trigger_2016-07-19T15:12:28.398352\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "curl localhost:8080/trigger/daily_processing/external_trigger_2016-07-19T15:12:28.398352\n",
    "{ \"dag_id\": \"daily_processing\",\n",
    "  \"execution_date\": \"2016-07-19T15:12:28\",\n",
    "  \"run_id\": \"external_trigger_2016-07-19T15:12:28.398352\",\n",
    "  \"state\": \"running\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deployment / What happens inside\n",
    "\n",
    "Two processes and a database\n",
    "- scheduler\n",
    "- webserver\n",
    "- database: postgres, sqlite(with restrictions), ...\n",
    "\n",
    "Executor: different possibilities exist\n",
    "- SequentialExecutor (within scheduler process)\n",
    "- LocalExecutor (with subprocesses)\n",
    "- Celery Framework (multiple worker nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How we use it\n",
    "- automatic and manual triggers\n",
    "- one airflow instance per system we manage\n",
    "- database: sometimes postgres, sometimes sqlite\n",
    "- lightweight executors, only triggers http requests\n",
    "- contributing to airflow with pull requests\n",
    " - external_triggers functionality (PR 503/540)\n",
    " - plugin detection mechanism (PR 730)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Challenges / Pitfalls\n",
    "- scheduling\n",
    "- start time and backfill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Scheduling\n",
    "\n",
    "![execution_date](images/execution_date.png)\n",
    "- start date: when did it start really\n",
    "- execution date: \n",
    " - more like a description for that run\n",
    " - always one iteration back in time\n",
    " - comes from ETL scenarios where data was available only on the next day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Start Time and Backfill\n",
    "\n",
    "- for every dag, you have to define a start time\n",
    "- if the dag has a schedule, the scheduler will trigger a backfill to that date\n",
    "\n",
    "![backfill](images/backfill.png)\n",
    "\n",
    "When you know the start time at design time of your dag, this is fine.\n",
    "\n",
    "If not, you have to take care what date to enter:\n",
    "- it should not be too much in the past, otherwise backfill will be triggered\n",
    "- ideally it should be one iteration before your first intended run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# If you want to dig deeper:\n",
    "\n",
    "\n",
    "[https://github.com/apache/incubator-airflow](https://github.com/apache/incubator-airflow)\n",
    "\n",
    "airflow documentation [http://pythonhosted.org/airflow/](http://pythonhosted.org/airflow/)\n",
    "\n",
    "common pitfalls (from airflow wiki) [https://cwiki.apache.org/confluence/display/AIRFLOW/Common+Pitfalls](https://cwiki.apache.org/confluence/display/AIRFLOW/Common+Pitfalls)\n",
    "\n",
    "plugin example from this talk: [https://github.com/blue-yonder/airflow-plugin-demo](https://github.com/blue-yonder/airflow-plugin-demo)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
